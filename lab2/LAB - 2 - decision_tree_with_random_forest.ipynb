{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeXkfWHYkTxK"
   },
   "source": [
    "# Machine Learning from Scratch : -Part 1- Decision Trees and Random Forests\n",
    "\n",
    "## 1. Aims and Structure\n",
    "\n",
    "This notebook will guide you through the implementation of Decision Trees and Random Forests from scratch. It is divided into three main sections:\n",
    "  \n",
    "> - **Utility Functions Section:**: Implements essential helper functions that support the core functionality of both algorithms.\n",
    "> - **Decision Tree Section:**: Which contains the main class of a Decision Tree classifier model\n",
    "> - **Random Forest Section:**: Implements an ensemble of Decision Trees to form a Random Forest model.\n",
    "---\n",
    "> **Note**: This implementation relies solely on the NumPy library. Please adhere to this standard.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfwL6we1kTxM"
   },
   "source": [
    "## 2. Theoretical Aspects\n",
    "\n",
    "> The Decision Tree Algorithm is an information-based model in machine learning used for both classification and regression tasks (Classification in this lab). It is constructed based on an **information metric**, which is why it is referred to as an information-based model.\n",
    "> In a Decision Tree, the model is structured as a hierarchy of nodes and arcs. Three types of nodes can be distinguished:\n",
    ">\n",
    ">> - ***Root node***: The first node in the tree, containing the entire dataset.\n",
    ">> - ***Interior nodes***: Intermediate nodes that help in decision-making but do not provide final classifications themselves.\n",
    ">> - ***Leaf nodes***: Terminal nodes without children that serve as decision nodes, providing the final classification or regression output.\n",
    "\n",
    "> ### Process\n",
    ">The process invovle the following steps:\n",
    ">\n",
    ">1. Compute the information gain for each feature (**column**) in the dataset.\n",
    ">2. Select the **feature** with the **highest** information gain.\n",
    ">3. Split the dataset based on the **selected feature**, creating subsets for each **unique value** of that feature.\n",
    ">4. Recursively repeat this process for each subset, considering only the **remaining features** at each step.\n",
    "\n",
    ">### Node Structure\n",
    ">\n",
    "> Each node in the tree contains the following **attributes**:\n",
    ">\n",
    ">***\n",
    ">- Dataset Sample: A subset of the dataset (for the root node, it contains the entire dataset), divided into **features** and **target** values (**vectors**).\n",
    ">- Decision Label: The most **frequent class label** in the dataset sample.\n",
    ">- Feature (**Column**): The feature used for splitting at that node.\n",
    ">- Depth: The level of the node within the tree.\n",
    ">- Subsets (**Children Nodes**): The resulting subsets after splitting, stored as a dictionary of nodes\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2Wvhu2mkTxM"
   },
   "source": [
    ">### Information gain metric\n",
    "\n",
    ">As mentioned earlier, Decision Trees use the Information Gain metric to determine the best feature for splitting the dataset and constructing nodes and leaves. To implement this metric effectively, it is essential to first understand its formula, meaning, and working mechanism.\n",
    "\n",
    ">Information Gain measures the amount of information obtained about the target variable when splitting the dataset based on a particular feature. It quantifies how much uncertainty (entropy) is **reduced** by considering that **feature**.\n",
    "\n",
    ">The uncertainty (or impurity) in the dataset is measured using Entropy, which is calculated as the weighted sum of the logarithms of the probabilities of each possible outcome (**class label**). In other words, entropy quantifies how impure or random the dataset is before and after a split.\n",
    "\n",
    ">The entropy formula is given by:\n",
    ">\n",
    "> H(t) = âˆ’ âˆ‘ (P(t = i) Ã— logs(P(t = i)))\n",
    "\n",
    "---\n",
    "\n",
    "Now, letâ€™s move on to the fun partâ€”**coding!** ðŸ˜ðŸš€\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYrSfqj9kTxN"
   },
   "source": [
    "# 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D__ei3jkTxN"
   },
   "source": [
    "> ## Utility Functions\n",
    "\n",
    "> The first method is **entropy** which is given the selected feature as a list (numpy array) named *column*.\n",
    "\n",
    "> - From *column*, get two arrays; the uniques **values** (possible values) and their **counts** (or frequency)\n",
    "> - Calculate the probability for each value in values (careful which array to use ðŸ˜‰)\n",
    "> - Calculate the log2 probabilities for each probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HG-hPDhNkTxN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(column):\n",
    "    values, counts = np.unique(column, return_counts=True)  # Get unique values and their counts\n",
    "    probabilities = counts / counts.sum()  # Compute probabilities\n",
    "    log2probabilities = np.log2(probabilities)  # Compute log base 2 of probabilities\n",
    "\n",
    "    return -np.dot(probabilities, log2probabilities)  # Compute entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9paDUcbkTxO"
   },
   "source": [
    "> Now you need to calculate the remainder entropy (weighted entropy) after using the selected feature for splitting\n",
    "> The method is given two arrays; column (feature) and y (target)\n",
    "> - From *column*, get two arrays; the uniques **values** (possible values) and their **counts** (or frequency)\n",
    "> - Calculate the **weights** which are the **probabilities** of each value in the column list\n",
    "> - Now for each value, get its **entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7WVUIASKkTxO"
   },
   "outputs": [],
   "source": [
    "# TODO: this method needs to return the reminder part of the information gain formula\n",
    "\n",
    "def rem(column, y):\n",
    "    # Get unique values in the column and their counts\n",
    "    values, counts = np.unique(column, return_counts=True)\n",
    "    # Calculate the weights as the probabilities of each unique value\n",
    "    weights = counts / len(column)\n",
    "    \n",
    "    # For each unique value, compute the entropy of the subset of y corresponding to that value\n",
    "    entropies = []\n",
    "    for val in values:\n",
    "        # Create a mask to filter y where column equals the current value\n",
    "        mask = (column == val)\n",
    "        subset_y = y[mask]\n",
    "        entropies.append(entropy(subset_y))\n",
    "    \n",
    "    # Return the weighted average of the entropies (remainder)\n",
    "    return np.dot(weights, entropies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTgDs7HtkTxO"
   },
   "source": [
    "> Now you just need to substract the **remainder entropy** (using the column) from the **total entropy** in the given node (using the target array).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wiCQCYgukTxP"
   },
   "outputs": [],
   "source": [
    "# TODO: this method implements the information gain formula\n",
    "def information_gain(column, y):\n",
    "    return entropy(y) - rem(column, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chEP4DICkTxP"
   },
   "source": [
    "> This function is used to get the most frequent label within a given list X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FvHiFdZ8kTxP"
   },
   "outputs": [],
   "source": [
    "def mode(x):\n",
    "    values, counts = np.unique(x, return_counts=True)  # Get unique elements and their frequencies\n",
    "    index = np.argmax(counts)  # Find the index of the highest count\n",
    "    return values[index]       # Return the element with the highest frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLYDutl3kTxP"
   },
   "source": [
    "> This function evaluates the performance of our model (How many correct predictions out of the total number of observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "QIdIumOfkTxP"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    # Convert to numpy arrays in case they aren't already\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # Calculate the fraction of predictions that are correct\n",
    "    return np.mean(y_pred == y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKrfKTclkTxP"
   },
   "source": [
    "> ## Decision Tree\n",
    "\n",
    "> Our Decision Tree will consist of two main classes; **DecisionTree Classifier** which is the main class responsible for constructing the tree, fitting (training) and predicting (classifying). The **Node Class** represents individual nodes within the tree.\n",
    "> Each DecisionTree classifier object will build a tree composed of multiple Node objects.\n",
    "\n",
    "> ### Node Class\n",
    "First, we define a Node class to represent each node in the Decision Tree.\n",
    "\n",
    ">> Refer to **Node Structure** subsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PyS1lVlWkTxP"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, column=None, label=None, data=None, target=None):\n",
    "        self.column = column    # The feature column used for splitting at this node\n",
    "        self.label = label      # The label for leaf nodes\n",
    "        self.data = data        # The data at this node\n",
    "        self.target = target    # The target values corresponding to the data\n",
    "        self.children = {}      # Initialize children as an empty dictionary\n",
    "\n",
    "    def __str__(self):\n",
    "        if not self.children:\n",
    "            return f\"[ Label: {self.label} ]\"\n",
    "        return str(self.children)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8DkCFHskTxQ"
   },
   "source": [
    "> ### Decision Tree Classifier Class\n",
    "> The DecisionTreeClassifier class requires a max_depth parameter, which defines the maximum depth the tree can reach. This helps control overfitting by limiting how deep the tree can grow.\n",
    "\n",
    "> - The ***fit method*** Constructs the decision tree, starting from the **root node** and *recursively* growing branches until reaching the **leaf nodes**.\n",
    "\n",
    "> - The **predict method** Classifies new instances by traversing the tree from the root node down to a leaf node. The path is determined by comparing feature values at each node.\n",
    "\n",
    "> - The **find_best_split method** Uses the *information_gain* utility function to evaluate each feature and select the one that provides the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WWm3GKoNkTxQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Decision Tree class\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10):\n",
    "        self.max_depth = max_depth  # Set maximum depth for the tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the root node with all data and targets.\n",
    "        self.root = Node(data=X, target=y)\n",
    "        # Stack holds tuples of (node, current_depth)\n",
    "        stack = [(self.root, 0)]\n",
    "        \n",
    "        while stack:\n",
    "            node, depth = stack.pop()  # Get the next node and its depth\n",
    "            # Set the node's label to the mode of its target values\n",
    "            node.label = mode(node.target)\n",
    "            \n",
    "            # If no more data to split on or maximum depth reached, skip splitting.\n",
    "            if 0 in node.data.shape or depth >= self.max_depth:\n",
    "                continue\n",
    "            \n",
    "            # Find the best feature to split on\n",
    "            best_split = self.__find_best_split(node.data, node.target)\n",
    "            node.column = best_split\n",
    "            \n",
    "            # Get the column data for the selected feature\n",
    "            column = node.data[:, best_split]\n",
    "            # Get unique values of the selected feature\n",
    "            values = np.unique(column)\n",
    "            \n",
    "            # For each unique value, create a child node\n",
    "            for value in values:\n",
    "                mask = (column == value)\n",
    "                # Remove the best_split column from the child's data\n",
    "                child_data = np.delete(node.data[mask], best_split, axis=1)\n",
    "                child_target = node.target[mask]\n",
    "                \n",
    "                # If no data remains after the split, create an empty child node with parent's label\n",
    "                if 0 in child_data.shape:\n",
    "                    child = Node(data=child_data, target=child_target)\n",
    "                    child.label = node.label\n",
    "                else:\n",
    "                    child = Node(data=child_data, target=child_target)\n",
    "                    child.label = mode(child_target)\n",
    "                    # Append child node with increased depth\n",
    "                    stack.append((child, depth + 1))\n",
    "                \n",
    "                # Set the child node corresponding to the feature value\n",
    "                node.children[value] = child\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Apply the private __predict method along axis=1 (row-wise)\n",
    "        return np.apply_along_axis(self.__predict, axis=1, arr=X)\n",
    "\n",
    "    def __predict(self, x):\n",
    "        x = x.copy()  # Create a copy of x to avoid modifying the original input\n",
    "        current_node = self.root  # Start traversal at the root\n",
    "        \n",
    "        # Traverse the tree until reaching a leaf node\n",
    "        while current_node.children:\n",
    "            col = current_node.column\n",
    "            # Get the value from the input corresponding to the feature used for splitting\n",
    "            value = x[col]\n",
    "            # Remove the used feature from x for subsequent splits\n",
    "            x = np.concatenate((x[:col], x[col+1:]), axis=0)\n",
    "            \n",
    "            # Get the corresponding child node for the value\n",
    "            temp = current_node.children.get(value, None)\n",
    "            # If no child node exists for that value, return the current node's label\n",
    "            if temp is None:\n",
    "                return current_node.label\n",
    "            # Otherwise, continue traversing down the tree\n",
    "            current_node = temp\n",
    "            \n",
    "        return current_node.label\n",
    "\n",
    "    def __find_best_split(self, X, y):\n",
    "        # Define a lambda that computes information gain for a column\n",
    "        f = lambda col: information_gain(col, y)\n",
    "        # Apply the lambda function to each column (feature) of X\n",
    "        gains = np.apply_along_axis(f, axis=0, arr=X)\n",
    "        # Return the index of the feature with the maximum information gain\n",
    "        return gains.argmax()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gff6uE9H0FY"
   },
   "source": [
    "## Question:\n",
    "\n",
    "What is the difference in functionality between the **predict** and the **__predict** methods ? n/\n",
    "\n",
    "The predict method is the public interface for making predictions on an entire dataset (or multiple samples), while the __predict method is a helper (private) method that makes a prediction for a single instance (a single row of features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVlBZ-SCkTxQ"
   },
   "source": [
    "---\n",
    "Now, we create our dataset, preprocess it, and then use our DecisionTreeClassifier to classify its instances.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nXIx94OxkTxQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of    ID  STREAM     SLOPE ELEVATION VEGETATION\n",
       "0   1   False     steep      high  chapparal\n",
       "1   2    True  moderate       low   riparian\n",
       "2   3    True     steep    medium   riparian\n",
       "3   4   False     steep    medium  chapparal\n",
       "4   5   False      flat      high    conifer\n",
       "5   6    True     steep   highest    conifer\n",
       "6   7    True     steep      high  chapparal>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = {\n",
    "    \"ID\": [1, 2, 3, 4, 5, 6, 7],\n",
    "    \"STREAM\": [False, True, True, False, False, True, True],\n",
    "    \"SLOPE\": [\"steep\", \"moderate\", \"steep\", \"steep\", \"flat\", \"steep\", \"steep\"],\n",
    "    \"ELEVATION\": [\"high\", \"low\", \"medium\", \"medium\", \"high\", \"highest\", \"high\"],\n",
    "    # target column:\n",
    "    \"VEGETATION\": [\"chapparal\", \"riparian\", \"riparian\", \"chapparal\", \"conifer\", \"conifer\", \"chapparal\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "AO4zvKwdkTxQ"
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"VEGETATION\"])\n",
    "y = df[\"VEGETATION\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WqaG4kbUkTxQ"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Zzv1pHVUkTxQ"
   },
   "outputs": [],
   "source": [
    "columns = X.columns.tolist()\n",
    "\n",
    "df_original = df.copy()\n",
    "\n",
    "df = df.to_numpy()\n",
    "\n",
    "X = X.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "__pshBfDkTxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {False: {'steep': {'high': [ Label: chapparal ]}}}, 2: {True: {'moderate': {'low': [ Label: riparian ]}}}, 3: {True: {'steep': {'medium': [ Label: riparian ]}}}, 4: {False: {'steep': {'medium': [ Label: chapparal ]}}}, 5: {False: {'flat': {'high': [ Label: conifer ]}}}, 6: {True: {'steep': {'highest': [ Label: conifer ]}}}, 7: {True: {'steep': {'high': [ Label: chapparal ]}}}}\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTree(max_depth=5)  # declare a Decision Tree Classifier with a max depth of 5\n",
    "\n",
    "clf.fit(X, y)  # fit the classifier with X and y\n",
    "\n",
    "print(clf.root)  # print the classifier's root node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "cA0JVEemkTxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X)  # predict using X\n",
    "\n",
    "print(accuracy(y_pred, y))  # print the accuracy using the accuracy function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBcQmiEjkTxQ"
   },
   "source": [
    "---\n",
    "\n",
    "Curious about how our classifier compares to Scikit-Learn's DecisionTreeClassifier? Let's import it and compare the results!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4Io66AnwkTxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- ELEVATION <= 1.50\n",
      "|   |--- ELEVATION <= 0.50\n",
      "|   |   |--- SLOPE <= 1.00\n",
      "|   |   |   |--- class: 2\n",
      "|   |   |--- SLOPE >  1.00\n",
      "|   |   |   |--- class: 0\n",
      "|   |--- ELEVATION >  0.50\n",
      "|   |   |--- class: 2\n",
      "|--- ELEVATION >  1.50\n",
      "|   |--- STREAM <= 0.50\n",
      "|   |   |--- class: 0\n",
      "|   |--- STREAM >  0.50\n",
      "|   |   |--- class: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Encode categorical features\n",
    "encoder = OrdinalEncoder()\n",
    "X_encoded = encoder.fit_transform(df_original.drop(columns=[\"VEGETATION\"]))\n",
    "\n",
    "# Encode target variable\n",
    "y_encoded = pd.factorize(df_original[\"VEGETATION\"])[0]\n",
    "\n",
    "# Train a DecisionTreeClassifier\n",
    "clf_sklearn = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "clf_sklearn.fit(X_encoded, y_encoded)\n",
    "\n",
    "# Print the decision tree structure\n",
    "from sklearn.tree import export_text\n",
    "print(export_text(clf_sklearn, feature_names=df_original.drop(columns=[\"VEGETATION\"]).columns.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "u_tLKfzokTxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- ID <= 2.50\n",
      "|   |--- ID <= 0.50\n",
      "|   |   |--- class: chapparal\n",
      "|   |--- ID >  0.50\n",
      "|   |   |--- class: riparian\n",
      "|--- ID >  2.50\n",
      "|   |--- ELEVATION <= 2.00\n",
      "|   |   |--- ID <= 5.50\n",
      "|   |   |   |--- class: conifer\n",
      "|   |   |--- ID >  5.50\n",
      "|   |   |   |--- class: chapparal\n",
      "|   |--- ELEVATION >  2.00\n",
      "|   |   |--- class: chapparal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Convert X back to a DataFrame for proper encoding\n",
    "X_df = pd.DataFrame(X, columns=columns)\n",
    "\n",
    "# Apply Ordinal Encoding to categorical features\n",
    "encoder = OrdinalEncoder()\n",
    "X_df[['STREAM', 'SLOPE', 'ELEVATION']] = encoder.fit_transform(X_df[['STREAM', 'SLOPE', 'ELEVATION']])\n",
    "\n",
    "# Convert back to NumPy for training\n",
    "X_encoded = X_df.to_numpy()\n",
    "\n",
    "# Fit the DecisionTreeClassifier\n",
    "clf_sklearn = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "clf_sklearn.fit(X_encoded, y)\n",
    "\n",
    "# Print the trained decision tree structure\n",
    "from sklearn.tree import export_text\n",
    "print(export_text(clf_sklearn, feature_names=columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "jYE1nzSzkTxR",
    "outputId": "fa53a515-cbfe-4231-99cd-b773baef56f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy using your custom function\n",
    "acc = accuracy(y_pred, y)\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rBzvKTOkTxR"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-UBJ6D5xdwI"
   },
   "source": [
    "Random Forest is an ensemble learning method that builds multiple decision trees and combines their outputs to improve accuracy and reduce overfitting. It follows the principle of Bootstrap Aggregation (Bagging) and Feature Randomization.\n",
    "\n",
    "\n",
    "*   **Bootstrap Sampling:** Each decision tree is trained on a random subset of data (rows are sampled with replacement).\n",
    "\n",
    "*   **Feature Subsampling:** Each tree is trained using only a random subset of features, reducing correlation between trees.\n",
    "\n",
    "*   **Majority Voting:** The final prediction is determined by taking the most common prediction across all trees.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "-RP1jnLnkTxR",
    "outputId": "1aeeb9f6-888a-4cf5-b087-26827c46b4b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85714286\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=10, max_depth=5, max_features=5):\n",
    "        self.n_estimators = n_estimators  # Number of decision trees\n",
    "        self.max_depth = max_depth  # Max depth for each decision tree\n",
    "        self.max_features = max_features  # Max number of features per tree\n",
    "        \n",
    "        self.estimators = []  # List of decision tree classifiers\n",
    "        self.selected_features = []  # List of selected feature indices for each tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Ensure max_features does not exceed number of actual features\n",
    "        self.max_features = min(self.max_features, X.shape[1])\n",
    "\n",
    "        # Ensure n_estimators does not exceed 2^max_features\n",
    "        self.n_estimators = min(self.n_estimators, 2 ** self.max_features)\n",
    "\n",
    "        # Initialize decision trees\n",
    "        self.estimators = [DecisionTree(max_depth=self.max_depth) for _ in range(self.n_estimators)]\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Sample rows with replacement (bootstrap sampling)\n",
    "            row_sample_idx = np.random.choice(X.shape[0], size=X.shape[0], replace=True)\n",
    "            # Sample columns without replacement\n",
    "            col_sample_idx = np.random.choice(X.shape[1], size=self.max_features, replace=False)\n",
    "            \n",
    "            self.selected_features.append(col_sample_idx)  # Store selected features\n",
    "            \n",
    "            # Create new training set with selected features\n",
    "            B = X[row_sample_idx][:, col_sample_idx]  \n",
    "            B_labels = y[row_sample_idx]\n",
    "\n",
    "            # Fit the decision tree on the sampled data\n",
    "            self.estimators[i].fit(B, B_labels)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []  # List to store predictions from all trees\n",
    "        \n",
    "        for estimator, features in zip(self.estimators, self.selected_features):\n",
    "            # Predict using the current decision tree on the selected features\n",
    "            pred = estimator.predict(X[:, features])\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Take the mode (majority vote) across all trees for each sample\n",
    "        return np.apply_along_axis(mode, axis=0, arr=np.array(predictions))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10, max_depth=5, max_features=5)\n",
    "rf.fit(X, y)  # Train the random forest\n",
    "y_pred = rf.predict(X)  # Make predictions\n",
    "\n",
    "acc = accuracy(y_pred, y)  # Compute accuracy\n",
    "print(f\"Accuracy: {acc:.8f}\")  # Print accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZzieS1YkTxR"
   },
   "source": [
    "---\n",
    "We will proceed into testing Random Forest implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aLw8F_qD__A"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "bC0bR1J8kTxR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of    ID  STREAM     SLOPE ELEVATION VEGETATION\n",
       "0   1   False     steep      high  chapparal\n",
       "1   2    True  moderate       low   riparian\n",
       "2   3    True     steep    medium   riparian\n",
       "3   4   False     steep    medium  chapparal\n",
       "4   5   False      flat      high    conifer\n",
       "5   6    True     steep   highest    conifer\n",
       "6   7    True     steep      high  chapparal>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"ID\": [1, 2, 3, 4, 5, 6, 7],\n",
    "    \"STREAM\": [False, True, True, False, False, True, True],\n",
    "    \"SLOPE\": [\"steep\", \"moderate\", \"steep\", \"steep\", \"flat\", \"steep\", \"steep\"],\n",
    "    \"ELEVATION\": [\"high\", \"low\", \"medium\", \"medium\", \"high\", \"highest\", \"high\"],\n",
    "    # Target column:\n",
    "    \"VEGETATION\": [\"chapparal\", \"riparian\", \"riparian\", \"chapparal\", \"conifer\", \"conifer\", \"chapparal\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXGFWQnFkTxR"
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"VEGETATION\"])\n",
    "y = df[\"VEGETATION\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "XNCvKE-LkTxR"
   },
   "outputs": [],
   "source": [
    "X = X[:, 1:]  # Removes the first column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "KYFEzqmikTxS"
   },
   "outputs": [],
   "source": [
    "# TODO: Get columns names' list\n",
    "columns = df.columns.tolist()\n",
    "\n",
    "# TODO: Create a copy of the original DataFrame\n",
    "df_original = df.copy()\n",
    "\n",
    "# TODO: Convert df to a NumPy array\n",
    "df = df.to_numpy()\n",
    "\n",
    "# TODO: Convert X to a NumPy array (excluding the target column)\n",
    "X = df[:, :-1]  # Assuming \"VEGETATION\" is the last column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlLdWifyEPqz"
   },
   "source": [
    "### Fitting and Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "11ttH9xkkTxS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a RandomForestClassifier Object\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=5, max_features=3)\n",
    "\n",
    "# TODO: Fit the classifier\n",
    "clf.fit(X, y)\n",
    "\n",
    "# TODO: Predict using X\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "# TODO: Print the accuracy (using the accuracy function that you worked on before)\n",
    "acc = accuracy(y_pred, y)\n",
    "print(f\"Accuracy: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzhjvq-08oaD"
   },
   "source": [
    "# Extra\n",
    "\n",
    "> If you examine our DecisionTree classifier, you'll notice that it only considers **categorical features** when splitting nodes.\n",
    "### Task\n",
    "\n",
    "> The Elevation attribute is a **continuous variable** with the following values: \"ELEVATION\": [3900, 300, 1500, 1200, 4450, 5000, 3000]. Modify the classifier to properly handle continuous features by implementing an appropriate splitting strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkfzlLBR-760"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "dm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
